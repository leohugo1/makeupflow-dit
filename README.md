    ğŸ’„ MakeupFlow-DiT: Latent Makeup Transfer with Flow Matching
Este repositÃ³rio contÃ©m a implementaÃ§Ã£o oficial do MakeupFlow-DiT, uma arquitetura de transferÃªncia de maquiagem baseada em Diffusion Transformers (DiT) e treinada via Flow Matching (I2I). O sistema utiliza um espaÃ§o latente comprimido para transferir estilos de maquiagem de uma imagem de referÃªncia para um rosto alvo em alta resoluÃ§Ã£o (512x512).

 ---
    ğŸš§ Status do Projeto: Treinamento Multietapa

O projeto utiliza um script unificado (train.py) para gerenciar as trÃªs fases crÃ­ticas de desenvolvimento:

**Fase 1: Treinamento do VAE** â€“ OtimizaÃ§Ã£o da reconstruÃ§Ã£o facial 512px usando CelebA-HQ com Perceptual Loss (LPIPS).

**Fase 2: Treinamento do StyleVit** â€“ ExtraÃ§Ã£o de embeddings de maquiagem via Vision Transformer e Triplet Margin Loss no dataset FFHQ-Makeup.[FASE ATUAL]

**Fase 3: Treinamento do DiT (Flow Matching)** â€“ Aprendizado da trajetÃ³ria linear entre o rosto limpo e maquiado.

 ---
    ğŸš€ Arquitetura TÃ©cnica
* **Diffusion Transformer (DiT)**: Implementado com blocos de Cross-Attention e ativaÃ§Ãµes SwiGLU (SwiGLUMP) para maior eficiÃªncia e estabilidade no aprendizado de fluxos.

* **StyleViT**: Um Vision Transformer que utiliza PEG (Positional Encoding Generator) para capturar texturas de maquiagem.

* **VAE (Variational Autoencoder)**: Estrutura robusta com ResnetBlocks e AttnBlocks. Utiliza um fator de escala latente de 0.18215 para compatibilidade com fluxos de difusÃ£o modernos.

---
    ğŸ› ï¸ Tecnologias e Bibliotecas

A stack tecnolÃ³gica foi selecionada para alta performance em GPUs como a RTX 3060:

**PyTorch 2.1+:** Suporte nativo a torch.compile e torch.amp (Mixed Precision).

**MLflow:** Rastreamento de experimentos e mÃ©tricas como SSIM e Loss em tempo real.

**LPIPS:** CÃ¡lculo de similaridade perceptual baseado em VGG para reconstruÃ§Ãµes de VAE ultra-nÃ­tidas.

**Hugging Face Datasets:** Pipelines de dados eficientes para FFHQ-Makeup e CelebA-HQ.

**Einops:** ManipulaÃ§Ã£o de tensores via einsum para operaÃ§Ãµes de patch e unpatch no DiT.

---



    ğŸ“¦ InstalaÃ§Ã£o
 ```
Bash
# Clone o repositÃ³rio
git clone https://github.com/leohugo1/makeupflow-dit.git
cd makeupflow-dit

# Instale as dependÃªncias otimizadas
pip install -r requirements.txt
```
    ğŸš€ Como Executar (Docker)
O projeto utiliza Docker para garantir que os drivers de GPU (NVIDIA) e o servidor de mÃ©tricas (MLflow) funcionem perfeitamente em qualquer mÃ¡quina.

1. Iniciar o Treinamento
Para iniciar o treino da fase padrÃ£o (VAE) e o servidor de mÃ©tricas, use:
```
Bash
docker-compose up
```
2. Alterar a Fase de Treinamento
O projeto estÃ¡ configurado para ler a fase desejada via variÃ¡vel de ambiente. VocÃª pode alterar entre train_vae, style_vit e train_dit de duas formas:

A. Via Terminal (Sem alterar arquivos)
* Windows (PowerShell):
```
$env:PHASE="style_vit"; docker-compose up
```
* Linux/macOS:
```
PHASE=style_vit docker-compose up
```
B. Via arquivo .env (Recomendado)

Crie um arquivo chamado .env na raiz do projeto e defina a fase:
```
PHASE=style_vit
```

    ğŸ“Š Monitoramento (MLflow)

Acompanhe as perdas (loss), gradientes e amostras visuais em tempo real:

* Acesse: http://localhost:5000

---

    InferÃªncia e Pesos

Os pesos prÃ©-treinados estarÃ£o disponÃ­veis no Hugging Face assim que as etapas de treinamento forem concluÃ­das: ğŸ”— Hugging Face: [MakeupFlow-DiT](https://huggingface.co/leonardohugo134/makeupflow-dit)